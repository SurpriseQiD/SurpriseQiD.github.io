<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>学术论文 on QiView</title><link>https://SurpriseQiD.github.io/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/</link><description>Recent content in 学术论文 on QiView</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Tue, 06 Jan 2026 04:24:52 -0500</lastBuildDate><atom:link href="https://SurpriseQiD.github.io/categories/%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87/index.xml" rel="self" type="application/rss+xml"/><item><title>基于深度强化学习的动态资源分配策略研究</title><link>https://SurpriseQiD.github.io/research/research-paper-1/</link><pubDate>Sat, 15 Nov 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-1/</guid><description>本文提出一种基于深度Q网络（DQN）的动态资源分配模型，旨在解决云计算环境中虚拟机资源的实时调度问题。实验结果表明，该模型相比传统启发式算法，在资源利用率和任务完成率上均有显著提升。</description></item><item><title>面向多智能体系统的去中心化协作策略研究</title><link>https://SurpriseQiD.github.io/research/research-paper-2/</link><pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-2/</guid><description>本文研究了在没有中央协调器的情况下，多个自主智能体如何学习协作以完成共同任务。我们提出了一种基于值函数分解的多智能体强化学习算法（QMIX），并将其应用于星际争霸II的微操场景中。结果表明，该方法能够学习到有效的去中心化协作策略，其性能超越了独立的Q学习和传统的规则方法。</description></item><item><title>基于图神经网络的金融风险传导模型研究</title><link>https://SurpriseQiD.github.io/research/research-paper-3/</link><pubDate>Fri, 05 Sep 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-3/</guid><description>本文构建了一个基于图神经网络（GNN）的金融系统性风险传导模型。通过将金融机构间的关联关系（如银行间借贷）建模为动态图，我们使用图注意力网络（GAT）来学习风险在网络中的传播模式。在对2008年金融危机期间的数据进行的回测实验中，该模型成功地识别出了关键的风险传播路径和系统重要性金融机构，其预测精度显著高于传统的网络中心性指标和向量自回归（VAR）模型。</description></item><item><title>基于自注意力机制的自然语言推理模型</title><link>https://SurpriseQiD.github.io/research/research-paper-4/</link><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-4/</guid><description>自然语言推理（NLI）是衡量机器语言理解能力的核心任务之一。本文提出了一种基于Transformer架构的深度学习模型，通过多头自注意力机制来捕捉前提（Premise）和假设（Hypothesis）之间的复杂语义关系。在斯坦福自然语言推理（SNLI）和多类型自然语言推理（MultiNLI）两个标准数据集上的实验表明，我们的模型在没有引入复杂外部知识的情况下，达到了与当时最先进模型相媲美的性能，证明了自注意力机制在建模句子对关系上的强大能力。</description></item><item><title>利用可解释AI（XAI）诊断和改进深度学习模型</title><link>https://SurpriseQiD.github.io/research/research-paper-5/</link><pubDate>Tue, 15 Jul 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-5/</guid><description>深度学习模型，特别是深度神经网络（DNNs），常被批评为“黑箱”，其决策过程不透明，难以理解和信任。本文探讨了如何利用可解释AI（XAI）技术，如LIME和SHAP，来诊断、调试和改进计算机视觉任务中的深度学习模型。我们通过一个案例研究，展示了如何使用XAI方法发现模型在训练数据中学到的“捷径”和偏见，并指导我们通过数据增强和模型结构调整来修复这些问题，从而提升模型的泛化能力和鲁棒性。</description></item></channel></rss>