<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="QiView"><meta property="og:type" content="article"><meta property="og:image" content="https://SurpriseQiD.github.io//img/home-bg-painting.jpg"><meta property="twitter:image" content="https://SurpriseQiD.github.io//img/home-bg-painting.jpg"><meta name=title content="强化学习：通过试错学习最优决策"><meta property="og:title" content="强化学习：通过试错学习最优决策"><meta property="twitter:title" content="强化学习：通过试错学习最优决策"><meta name=description content="QiView — 一个专注于计算与决策科学的学术知识平台，提供博弈论、机器学习、因果推断等领域的系统化知识。"><meta property="og:description" content="QiView — 一个专注于计算与决策科学的学术知识平台，提供博弈论、机器学习、因果推断等领域的系统化知识。"><meta property="twitter:description" content="QiView — 一个专注于计算与决策科学的学术知识平台，提供博弈论、机器学习、因果推断等领域的系统化知识。"><meta property="twitter:card" content="强化学习是机器学习的一个独特分支，它关注智能体如何在与环境的互动中，通过试错来学习一系列能够最大化累积奖励的行动。本文将介绍强化学习的核心框架——马尔可夫决策过程（MDP），以及经典的Q-learning算法。"><meta name=keyword content="博弈论, 决策科学, 运营管理, 人机交互, 机器学习, 因果推断, 大语言模型"><link rel="shortcut icon" href=/img/favicon.png><title>强化学习：通过试错学习最优决策 | QiView | 计算与决策科学知识平台</title><link rel=canonical href=/knowledge/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ml-reinforcement-learning/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><link rel=stylesheet href=https://SurpriseQiD.github.io/css/custom.css><link rel=stylesheet href=https://SurpriseQiD.github.io/css/resources.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-CX84QDN0SR"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-CX84QDN0SR")}</script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=#navbar-main aria-expanded=false>
<span class=sr-only>切换导航</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/><span class=brand-text>QiView</span></a></div><div class="collapse navbar-collapse" id=navbar-main><ul class="nav navbar-nav navbar-right"><li><a href=/>首页</a></li><li><a href=/knowledge/>知识库</a></li><li><a href=/research/>研究</a></li><li><a href=/blog/>博客</a></li><li><a href=/resources/>资源</a></li><li><a href=/search/>搜索</a></li><li><a href=/about/>关于</a></li><li><a href=/search class=nav-search><i class="fa fa-search"></i></a></li></ul></div></div></nav><script>document.addEventListener("DOMContentLoaded",function(){var n,e=document.querySelector(".navbar-toggle"),t=document.querySelector(".navbar-collapse");e&&e.addEventListener("click",function(){this.classList.toggle("collapsed"),t.classList.toggle("in");var e=this.getAttribute("aria-expanded")==="true";this.setAttribute("aria-expanded",!e)}),n=document.querySelectorAll(".navbar-nav a"),n.forEach(function(n){n.addEventListener("click",function(){window.innerWidth<992&&(t.classList.remove("in"),e&&(e.classList.add("collapsed"),e.setAttribute("aria-expanded","false")))})}),document.addEventListener("click",function(n){var s=n.target.closest(".navbar");!s&&window.innerWidth<992&&(t.classList.remove("in"),e&&(e.classList.add("collapsed"),e.setAttribute("aria-expanded","false")))})})</script><header class=intro-header style=background-image:url(/img/home-bg-painting.jpg)><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=site-heading><h1>QiView</h1><span class=subheading>构建系统化的学术知识生态系统</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-1
col-md-8 col-md-offset-1
col-sm-12
col-xs-12
post-container"><h2 id=1-什么是强化学习>1. 什么是强化学习？</h2><p>想象一下训练一只宠物狗。当你发出“坐下”的指令，如果它做对了，你会给它一块零食（正向奖励）；如果它做错了，你可能会忽略它或说“不对”（没有奖励或负向奖励）。经过多次尝试，狗会逐渐学会“坐下”这个指令与获得奖励之间的联系。强化学习（Reinforcement Learning, RL）的灵感就来源于此。</p><blockquote><p><strong>定义</strong>：强化学习是机器学习的一个领域，研究智能体（Agent）如何在一个环境（Environment）中采取行动（Action），以最大化随时间累积的奖励（Reward）。</p></blockquote><p>与监督学习不同，强化学习没有现成的“正确答案”标签。智能体必须自己去探索环境，通过“试错”（Trial and Error）来发现哪些行动能够带来长期最好的结果。这使得强化学习特别适用于解决需要连续决策的复杂问题。</p><h2 id=2-核心框架马尔可夫决策过程-mdp>2. 核心框架：马尔可夫决策过程 (MDP)</h2><p>强化学习问题通常被建模为<strong>马尔可夫决策过程（Markov Decision Process, MDP）</strong>。一个MDP由以下五个核心元素组成：</p><ul><li><strong>状态 (State, S)</strong>：对环境在某一时刻的描述。例如，在棋类游戏中，当前棋盘的布局就是一个状态。</li><li><strong>动作 (Action, A)</strong>：智能体可以采取的行动。例如，在棋盘上移动一个棋子。</li><li><strong>转移概率 (Transition Probability, P)</strong>：在某个状态s下采取动作a后，转移到下一个状态s&rsquo;的概率。它描述了环境的动态性。</li><li><strong>奖励 (Reward, R)</strong>：在某个状态s下采取动作a后，环境给予智能体的即时反馈。奖励可以是正的，也可以是负的。</li><li><strong>贴现因子 (Discount Factor, γ)</strong>：一个介于0和1之间的值，用于平衡即时奖励和未来奖励的重要性。γ越小，智能体越“短视”；γ越大，智能体越有“远见”。</li></ul><p>智能体的目标是学习一个<strong>策略（Policy, π）</strong>。策略是一个从状态到动作的映射，它告诉智能体在每个状态下应该采取什么行动。最优策略就是能够最大化累去积贴现奖励（即“回报”）的策略。</p><h2 id=3-价值函数与q函数>3. 价值函数与Q函数</h2><p>为了找到最优策略，我们需要评估在某个状态下或在某个状态下采取某个行动有多“好”。这通过价值函数来衡量。</p><ul><li><strong>状态价值函数 V(s)</strong>：表示从状态s开始，遵循某个策略π，所能获得的期望回报。</li><li><strong>动作价值函数 Q(s, a)</strong>：也称为Q函数，表示在状态s下，采取动作a，然后继续遵循策略π，所能获得的期望回报。Q函数是强化学习中更常用的一个概念，因为它直接告诉我们在一个状态下选择哪个动作更好。</li></ul><p>最优策略与最优Q函数（Q*）之间有一个直接的关系：一旦我们知道了最优Q函数，最优策略就是在任何状态s下，选择那个能使Q*(s, a)最大化的动作a。</p><h2 id=4-经典算法q-learning>4. 经典算法：Q-Learning</h2><p>那么，我们如何学习到这个最优的Q函数呢？Q-Learning是一种经典的时序差分（Temporal Difference, TD）学习算法，它允许智能体在没有环境模型（即不知道转移概率P和奖励函数R）的情况下，通过与环境的互动来逐步逼近最优Q函数。</p><p>Q-Learning的更新规则如下：</p><p><code>Q(s, a) ← Q(s, a) + α * [r + γ * max_{a'} Q(s', a') - Q(s, a)]</code></p><p>让我们来分解这个公式：</p><ul><li><code>s, a</code>：当前状态和采取的动作。</li><li><code>r, s'</code>：执行动作a后，获得的即时奖励和进入的新状态。</li><li><code>α</code>：学习率（Learning Rate），控制每次更新的步长。</li><li><code>r + γ * max_{a'} Q(s', a')</code>：这是我们对Q(s, a)的新的、更好的估计，被称为“TD目标”。它由即时奖励r和对未来最大回报的估计（贴现后的新状态s&rsquo;的最大Q值）组成。</li><li><code>[ ... - Q(s, a)]</code>：这部分被称为“TD误差”，即新的估计与旧的估计之间的差距。Q-Learning的目标就是不断减小这个误差。</li></ul><p>智能体通过在环境中不断地探索（尝试新的动作）和利用（选择当前最好的动作），并使用上述规则来更新其Q表（一个存储所有状态-动作对Q值的表格），最终Q表会收敛到最优的Q*。</p><h2 id=5-深度强化学习-deep-reinforcement-learning>5. 深度强化学习 (Deep Reinforcement Learning)</h2><p>在现实世界的许多问题中（如视频游戏、机器人控制），状态空间和动作空间极其巨大，用一个表格来存储所有Q值是不现实的。深度强化学习（DRL）通过使用深度神经网络来近似Q函数（或价值函数、策略），解决了这个问题。DeepMind的DQN（Deep Q-Network）就是将Q-Learning与深度学习结合的典范，它成功地在雅达利（Atari）游戏上达到了超越人类水平的表现。</p><h2 id=6-应用与展望>6. 应用与展望</h2><p>强化学习已经在许多领域取得了突破性进展：</p><ul><li><strong>游戏AI</strong>：AlphaGo击败世界围棋冠军。</li><li><strong>机器人控制</strong>：训练机器人完成复杂的抓取和行走任务。</li><li><strong>推荐系统</strong>：动态地为用户推荐内容以最大化长期用户参与度。</li><li><strong>资源优化</strong>：数据中心的能源管理、交通信号灯的智能控制。</li></ul><p>强化学习是一个充满活力和挑战的领域，它被认为是通向通用人工智能（AGI）的关键路径之一。</p><script src=https://giscus.app/client.js data-repo=SurpriseQiD/hugo-main data-repo-id data-category=Announcements data-category-id data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class="col-lg-3 col-md-3 col-sm-12
sidebar-column"><div class=sidebar-container><div class=short-about><div class=sidebar-avatar><a href=/about><img src=/img/avatar-patrickStar.jpg alt=avatar></a></div><p class=sidebar-description>专注于计算与决策科学的学术知识分享平台</p><div class="sidebar-social inline"><a href=mailto:dongq@mail.ustc.edu.cn title=Email aria-label=Email><i class="fas fa-envelope"></i>
</a><a target=_blank href=https://github.com/SurpriseQiD title=GitHub aria-label=GitHub><i class="fab fa-github"></i>
</a><a target=_blank href=https://www.researchgate.net/profile/Qi-Dong-20 title=ResearchGate aria-label=ResearchGate><i class="fab fa-researchgate"></i>
</a><a target=_blank href="https://scholar.google.com/citations?user=dEDX5coAAAAJ&amp;hl=zh-CN&amp;oi=sra" title="Google Scholar" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i>
</a><a target=_blank href=https://orcid.org/0000-0002-3052-6332 title=ORCID aria-label=ORCID><i class="fab fa-orcid"></i></a></div></div><div class="sidebar-module toc-module"><h4><i class="fas fa-list-alt"></i> 目录</h4><nav id=TableOfContents><nav id=TableOfContents><ul><li><a href=#1-什么是强化学习>1. 什么是强化学习？</a></li><li><a href=#2-核心框架马尔可夫决策过程-mdp>2. 核心框架：马尔可夫决策过程 (MDP)</a></li><li><a href=#3-价值函数与q函数>3. 价值函数与Q函数</a></li><li><a href=#4-经典算法q-learning>4. 经典算法：Q-Learning</a></li><li><a href=#5-深度强化学习-deep-reinforcement-learning>5. 深度强化学习 (Deep Reinforcement Learning)</a></li><li><a href=#6-应用与展望>6. 应用与展望</a></li></ul></nav></nav></div></div></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:dongq@mail.ustc.edu.cn><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/SurpriseQiD><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title=QiView><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; QiView 2026</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><span id=total-views class=site-counter><i class="fa fa-eye"></i>
<span class=leancloud-total-views></span>
</span><script>function showTotalViews(){var e=AV.Object.extend("SiteCounter"),t=new AV.Query(e);t.first().then(function(e){var t=e?e.get("totalViews"):0;document.querySelector(".leancloud-total-views").textContent=t})}showTotalViews()</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>