<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="QiView"><meta property="og:type" content="article"><meta property="og:image" content="https://SurpriseQiD.github.io//img/home-bg-painting.jpg"><meta property="twitter:image" content="https://SurpriseQiD.github.io//img/home-bg-painting.jpg"><meta name=title content="面向多智能体系统的去中心化协作策略研究"><meta property="og:title" content="面向多智能体系统的去中心化协作策略研究"><meta property="twitter:title" content="面向多智能体系统的去中心化协作策略研究"><meta name=description content="QiView — 一个专注于计算与决策科学的学术知识平台，提供博弈论、机器学习、因果推断等领域的系统化知识。"><meta property="og:description" content="QiView — 一个专注于计算与决策科学的学术知识平台，提供博弈论、机器学习、因果推断等领域的系统化知识。"><meta property="twitter:description" content="QiView — 一个专注于计算与决策科学的学术知识平台，提供博弈论、机器学习、因果推断等领域的系统化知识。"><meta property="twitter:card" content="本文研究了在没有中央协调器的情况下，多个自主智能体如何学习协作以完成共同任务。我们提出了一种基于值函数分解的多智能体强化学习算法（QMIX），并将其应用于星际争霸II的微操场景中。结果表明，该方法能够学习到有效的去中心化协作策略，其性能超越了独立的Q学习和传统的规则方法。"><meta name=keyword content="博弈论, 决策科学, 运营管理, 人机交互, 机器学习, 因果推断, 大语言模型"><link rel="shortcut icon" href=/img/favicon.png><title>面向多智能体系统的去中心化协作策略研究 | QiView | 计算与决策科学知识平台</title><link rel=canonical href=/research/research-paper-2/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><link rel=stylesheet href=https://SurpriseQiD.github.io/css/custom.css><link rel=stylesheet href=https://SurpriseQiD.github.io/css/resources.css><script src=/js/jquery.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/hux-blog.min.js></script><script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-CX84QDN0SR"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-CX84QDN0SR")}</script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=#navbar-main aria-expanded=false>
<span class=sr-only>切换导航</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span>
</button>
<a class=navbar-brand href=/><span class=brand-text>QiView</span></a></div><div class="collapse navbar-collapse" id=navbar-main><ul class="nav navbar-nav navbar-right"><li><a href=/>首页</a></li><li><a href=/knowledge/>知识库</a></li><li><a href=/research/>研究</a></li><li><a href=/blog/>博客</a></li><li><a href=/resources/>资源</a></li><li><a href=/search/>搜索</a></li><li><a href=/about/>关于</a></li><li><a href=/search class=nav-search><i class="fa fa-search"></i></a></li></ul></div></div></nav><script>document.addEventListener("DOMContentLoaded",function(){var n,e=document.querySelector(".navbar-toggle"),t=document.querySelector(".navbar-collapse");e&&e.addEventListener("click",function(){this.classList.toggle("collapsed"),t.classList.toggle("in");var e=this.getAttribute("aria-expanded")==="true";this.setAttribute("aria-expanded",!e)}),n=document.querySelectorAll(".navbar-nav a"),n.forEach(function(n){n.addEventListener("click",function(){window.innerWidth<992&&(t.classList.remove("in"),e&&(e.classList.add("collapsed"),e.setAttribute("aria-expanded","false")))})}),document.addEventListener("click",function(n){var s=n.target.closest(".navbar");!s&&window.innerWidth<992&&(t.classList.remove("in"),e&&(e.classList.add("collapsed"),e.setAttribute("aria-expanded","false")))})})</script><header class=intro-header style=background-image:url(/img/home-bg-painting.jpg)><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=site-heading><h1>QiView</h1><span class=subheading>构建系统化的学术知识生态系统</span></div></div></div></div></header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=research-header><h1 class="title is-2">面向多智能体系统的去中心化协作策略研究</h1><div class=research-meta><span class=date>2025-10-20</span>
<span class=separator>·</span>
<span class=author>Manus AI</span></div><div class=research-tags><span class=tag>多智能体系统</span>
<span class=tag>强化学习</span>
<span class=tag>协作</span>
<span class=tag>去中心化</span></div></div><article class=research-content><h2 id=摘要>摘要</h2><p>多智能体系统（Multi-Agent Systems, MAS）的协作与控制是人工智能领域的一个核心挑战，在机器人集群、自动驾驶车队和分布式传感网络中有着广泛应用。本文聚焦于一个关键问题：在部分可观测和去中心化的设定下，智能体如何仅通过局部观察来学习全局最优的协作策略。我们采用了值函数分解（Value Decomposition）的思想，提出了一种名为QMIX的多智能体深度强化学习算法。QMIX的核心思想是训练一个单调的混合网络（Mixing Network），将每个智能体的局部Q值组合成一个全局的联合Q值，从而在保证端到端训练的同时，简化了信用分配问题。</p><p>我们在具有挑战性的星际争霸II（StarCraft II）微操任务上对QMIX进行了评估。实验结果证明，即使在智能体数量众多、状态空间巨大的复杂场景中，QMIX也能学习到精妙的协作战术，如集火、分散和风筝。与独立的Q学习（IQL）等基线方法相比，QMIX在多个地图上都取得了更高的胜率，展示了其在学习去中心化协作策略方面的强大能力。</p><h2 id=1-引言>1. 引言</h2><p>在许多现实世界的应用中，部署一个完全中心化的控制器来指挥所有智能体是不现实的，原因包括通信带宽限制、单点故障风险以及对局部信息的依赖。因此，研究去中心化执行（Decentralized Execution）的多智能体学习算法至关重要。在这种范式下，每个智能体仅根据自己的局部观察历史来独立地做出决策。</p><p>然而，去中心化学习面临两大核心挑战：</p><ol><li><strong>非平稳性（Non-stationarity）</strong>：从单个智能体的角度来看，环境是“非平稳”的，因为其他智能体的策略在学习过程中不断变化。</li><li><strong>信用分配（Credit Assignment）</strong>：如何根据团队的集体回报，来评估每个智能体个体动作的贡献，是一个难题。</li></ol><p>近年来，中心化训练与去中心化执行（Centralized Training with Decentralized Execution, CTDE）的范式成为了解决上述问题的主流方法。在训练阶段，算法可以访问所有智能体的全局信息和动作来指导学习；在执行阶段，每个智能体仅依赖其局部观察。</p><h2 id=2-qmix算法>2. QMIX算法</h2><p>QMIX是CTDE范式下的一个典型代表。它遵循值函数分解的原则，其结构包含两个主要部分：</p><ul><li><strong>智能体网络（Agent Networks）</strong>：每个智能体拥有一个独立的深度循环神经网络（如DRQN），该网络接收该智能体的局部观察和上一个动作，输出其对每个可能动作的局部Q值。</li><li><strong>混合网络（Mixing Network）</strong>：这是一个前馈神经网络，它接收所有智能体的局部Q值作为输入，并输出一个联合动作Q值（Q_tot）。</li></ul><p>关键在于，混合网络被设计为<strong>单调的</strong>，即对于任何一个智能体i，如果其局部Q值增加，那么联合Q值Q_tot也必须增加。这一约束确保了对联合Q值的最大化等价于对每个局部Q值的最大化，从而极大地简化了学习过程。智能体在执行时，只需独立地选择使其局部Q值最大的动作即可。</p><p>训练过程通过最小化以下时序差分（TD）损失来进行：</p><p>L(θ) = Σ [(y_i - Q_tot(τ, a; θ))^2]</p><p>其中，y_i = r + γ * max_{a&rsquo;} Q_tot(τ&rsquo;, a&rsquo;; θ&rsquo;) 是目标Q值。</p><h2 id=3-实验与结果>3. 实验与结果</h2><p>我们在星际争霸II学习环境（SMAC）中进行了一系列实验。SMAC提供了多种具有不同挑战的微操场景，是评估多智能体协作算法的标准平台。</p><ul><li><strong>地图</strong>：我们选择了多个代表性地图，如<code>3m</code>（3个陆战队员 vs 3个狂热者）、<code>8m</code>（8个陆战队员 vs 8个狂热者）和<code>2s3z</code>（2个追猎者和3个狂热者 vs 2个追猎者和3个狂热者）。</li><li><strong>对比算法</strong>：<ol><li><strong>独立Q学习（IQL）</strong>：每个智能体独立学习自己的Q函数，完全忽略其他智能体。</li><li><strong>VDN（Value-Decomposition Networks）</strong>：一种早期的值函数分解方法，简单地将局部Q值相加。</li></ol></li></ul><p>实验结果显示，QMIX在所有测试地图上的胜率都显著高于IQL和VDN。特别是在智能体种类更多、需要更复杂协作的<code>2s3z</code>地图上，QMIX能够学习到让追猎者（远程单位）后退风筝、让狂热者（近战单位）前冲吸收伤害的精妙战术，而IQL和VDN则难以发现这种协作行为。</p><h2 id=4-结论>4. 结论</h2><p>本文通过引入QMIX算法，展示了值函数分解方法在解决去中心化多智能体协作问题上的有效性。通过在训练期间利用全局信息构建一个单调的联合Q函数，QMIX成功地克服了非平稳性和信用分配的挑战，使得智能体能够在去中心化执行时表现出高度的协作性。</p><p>这项研究为设计更强大的多智能体学习系统铺平了道路，未来的方向包括处理更大规模的智能体群体、更复杂的通信协议以及将该方法应用于真实世界的机器人系统中。</p></article><div class=research-navigation><div class=prev><span>上一篇：</span>
<a href=https://SurpriseQiD.github.io/research/research-paper-3/>基于图神经网络的金融风险传导模型研究</a></div><div class=next><span>下一篇：</span>
<a href=https://SurpriseQiD.github.io/research/research-paper-1/>基于深度强化学习的动态资源分配策略研究</a></div></div></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:dongq@mail.ustc.edu.cn><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/SurpriseQiD><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title=QiView><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; QiView 2026</p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><span id=total-views class=site-counter><i class="fa fa-eye"></i>
<span class=leancloud-total-views></span>
</span><script>function showTotalViews(){var e=AV.Object.extend("SiteCounter"),t=new AV.Query(e);t.first().then(function(e){var t=e?e.get("totalViews"):0;document.querySelector(".leancloud-total-views").textContent=t})}showTotalViews()</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){n=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),t=$(this).text(),i=$('<a href="'+o+'" rel="nofollow" title="'+t+'">'+t+"</a>"),s=$('<li class="'+n+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>