<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>自注意力机制 on QiView</title><link>https://SurpriseQiD.github.io/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link><description>Recent content in 自注意力机制 on QiView</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Tue, 06 Jan 2026 04:24:52 -0500</lastBuildDate><atom:link href="https://SurpriseQiD.github.io/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.xml" rel="self" type="application/rss+xml"/><item><title>基于自注意力机制的自然语言推理模型</title><link>https://SurpriseQiD.github.io/research/research-paper-4/</link><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-4/</guid><description>自然语言推理（NLI）是衡量机器语言理解能力的核心任务之一。本文提出了一种基于Transformer架构的深度学习模型，通过多头自注意力机制来捕捉前提（Premise）和假设（Hypothesis）之间的复杂语义关系。在斯坦福自然语言推理（SNLI）和多类型自然语言推理（MultiNLI）两个标准数据集上的实验表明，我们的模型在没有引入复杂外部知识的情况下，达到了与当时最先进模型相媲美的性能，证明了自注意力机制在建模句子对关系上的强大能力。</description></item></channel></rss>