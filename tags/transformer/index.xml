<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer on QiView</title><link>https://SurpriseQiD.github.io/tags/transformer/</link><description>Recent content in Transformer on QiView</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Tue, 06 Jan 2026 04:24:52 -0500</lastBuildDate><atom:link href="https://SurpriseQiD.github.io/tags/transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>大型语言模型：原理、架构与应用</title><link>https://SurpriseQiD.github.io/knowledge/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/llm-foundations/</link><pubDate>Fri, 05 Dec 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/knowledge/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/llm-foundations/</guid><description>大型语言模型的核心原理、Transformer架构详解及其在现代NLP中的应用</description></item><item><title>基于自注意力机制的自然语言推理模型</title><link>https://SurpriseQiD.github.io/research/research-paper-4/</link><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate><guid>https://SurpriseQiD.github.io/research/research-paper-4/</guid><description>自然语言推理（NLI）是衡量机器语言理解能力的核心任务之一。本文提出了一种基于Transformer架构的深度学习模型，通过多头自注意力机制来捕捉前提（Premise）和假设（Hypothesis）之间的复杂语义关系。在斯坦福自然语言推理（SNLI）和多类型自然语言推理（MultiNLI）两个标准数据集上的实验表明，我们的模型在没有引入复杂外部知识的情况下，达到了与当时最先进模型相媲美的性能，证明了自注意力机制在建模句子对关系上的强大能力。</description></item></channel></rss>